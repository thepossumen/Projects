{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "e4NHHmA6KF3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "AYZ_iKfTKxd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9UFnTEKJy9d"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "dataset = load_dataset(\"anordkvist/gu-course-syllabus\")\n",
        "df = dataset['train'].to_pandas()\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list of titles (section headers) in the order they appear in the documents\n",
        "titles = [\n",
        "    \"Confirmation\",\n",
        "    \"Position in the educational system\",\n",
        "    \"Entry requirements\",\n",
        "    \"Learning outcomes\",\n",
        "    \"Course content\",\n",
        "    \"Form of teaching\",\n",
        "    \"Assessment\",\n",
        "    \"Grades\",\n",
        "    \"Course evaluation\",\n",
        "    \"Additional information\"\n",
        "]\n",
        "\n",
        "num_titles = len(titles)\n",
        "\n",
        "# make sure that padding tokens isnt included when creating the embedding\n",
        "def average_pool(last_hidden_states: Tensor,\n",
        "                 attention_mask: Tensor) -> Tensor:\n",
        "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "\n",
        "def create_section_embeddings(model_name, df_docs, save_path=None):\n",
        "\n",
        "  \"\"\"\n",
        "  Creates embedding for each section in the document.\n",
        "  \"\"\"\n",
        "  # init model and tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModel.from_pretrained(model_name)\n",
        "  # move to gpu if available\n",
        "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  model.to(device)\n",
        "\n",
        "  # def create_embedding(text_batch):\n",
        "  #     # convert series into list of strings\n",
        "  #     input_texts = text_batch.tolist()\n",
        "  #     # Tokenize the input texts\n",
        "  #     batch_dict = tokenizer(input_texts, max_length=256, padding=True, truncation=True, return_tensors='pt')\n",
        "  #     # create embeddings\n",
        "  #     outputs = model(**batch_dict)\n",
        "  #     embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
        "  #     # normalize embeddings\n",
        "  #     embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "  #     return embeddings\n",
        "\n",
        "  def create_embedding(text_batch):\n",
        "    with torch.no_grad(): # This ensures that gradients are not tracked\n",
        "      # convert series into list of strings\n",
        "      input_texts = text_batch.tolist()\n",
        "      # Tokenize the input texts\n",
        "      batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "      # Move batch_dict to GPU\n",
        "      batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
        "      # create embeddings\n",
        "      outputs = model(**batch_dict)\n",
        "      embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
        "      # normalize embeddings\n",
        "      embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "      # Free up memory\n",
        "      del batch_dict\n",
        "      del outputs\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    return embeddings.cpu()  # Move embeddings to CPU to free up GPU memory\n",
        "\n",
        "\n",
        "  embeddings = []\n",
        "\n",
        "  print(f'Creating embeddings...')\n",
        "  # iterate over each document at a time, this will process each document as a batch (we might want to have a larger batch size, in that case we would rework this a little bit)\n",
        "  for _, document in tqdm(df_docs.iterrows(), total=df_docs.shape[0]):\n",
        "      # exclude course code column\n",
        "      sections = document[1:]\n",
        "      # create embeddings for each document\n",
        "      section_embeddings = create_embedding(sections)\n",
        "      # this list will contain all the section embeddings, where the first 10 belongs to the first document, next 10 belongs to the second...\n",
        "      embeddings.extend(section_embeddings)\n",
        "\n",
        "  # init dict to store section embeddings\n",
        "  d_section_embeddings = {}\n",
        "\n",
        "  # iterate over the list of embeddings, map embeddings to course codes and section titles\n",
        "  for index, course_code in enumerate(df_docs['course_codes']):\n",
        "      # calculate the start and end index for the embeddings of this course\n",
        "      start_idx = index * num_titles\n",
        "      end_idx = start_idx + num_titles\n",
        "\n",
        "      # map section titles to their embedding\n",
        "      sections_embeddings = dict(zip(titles, embeddings[start_idx:end_idx]))\n",
        "\n",
        "      # Add this to your dictionary\n",
        "      d_section_embeddings[course_code] = sections_embeddings\n",
        "\n",
        "  # if save_path:\n",
        "  #     print(f'Saving section embeddings to: {save_path}')\n",
        "  #     with open(save_path+'section_embeddings.pickle', 'wb') as f:\n",
        "  #         pickle.dump(d_section_embeddings, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return d_section_embeddings"
      ],
      "metadata": {
        "id": "0UkBRpdiKYit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'intfloat/e5-large-v2'\n",
        "\n",
        "dict_section_embeddings = create_section_embeddings(model_name, df)"
      ],
      "metadata": {
        "id": "axs9hUeJKY4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remake nested dict to df to be able to save in hf\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(dict_section_embeddings, orient='index').reset_index()\n",
        "df = df.rename(columns={'index': 'course_code'})\n",
        "\n",
        "# convert pytorch tensors to numpy arrays to python lists (hf reasons), skip the first course_code column\n",
        "for column in df.columns[1:]:\n",
        "    # Convert each tensor to a numpy array, then to a list\n",
        "    df[column] = df[column].apply(lambda x: x.numpy().tolist()) # maybe we dont need to make it to list, migh work with np array"
      ],
      "metadata": {
        "id": "pjklLQwLKZau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# push embeddings to anordkvist/gu-course-syllabus-embeddings\n",
        "from huggingface_hub import login\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "login('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')\n",
        "\n",
        "dataset.push_to_hub(repo_id='anordkvist/gu-course-syllabus-embeddings')"
      ],
      "metadata": {
        "id": "Xa7g5lexKZjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"anordkvist/gu-course-syllabus\")\n",
        "df_hf = dataset['train'].to_pandas()"
      ],
      "metadata": {
        "id": "jqW2QAgj-0hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hf.head()"
      ],
      "metadata": {
        "id": "7sairIvq-5tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}