{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI api key\n",
    "key = ''\n",
    "# set the env variable\n",
    "os.environ['OPENAI_API_KEY'] = key\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_pairs(system_eval_path):\n",
    "    \"\"\"\n",
    "    Use the system eval json file.\n",
    "    \"\"\"\n",
    "    with open(system_eval_path, 'r') as file:\n",
    "        system_eval = json.load(file)\n",
    "    \n",
    "    # get the user queries/outer keys\n",
    "    queries = list(system_eval.keys())\n",
    "    # get the models/inner keys, remove 'query type', 'truth' and 'context'\n",
    "    inner_keys = list(system_eval[queries[0]].keys())[3:]\n",
    "\n",
    "    # generate qa pairs\n",
    "    qa_pairs = []\n",
    "    # iterate over each model\n",
    "    for model in inner_keys:\n",
    "        model_responses = []\n",
    "        # iterate over each query\n",
    "        for query in queries:\n",
    "            # get the answer for the specific model on each query\n",
    "            response = system_eval[query][model]\n",
    "            # get the context\n",
    "            context = system_eval[query]['context']\n",
    "            model_responses.append(f'Question: {query}, Context: {context}, Answer: {response}')\n",
    "        # append all models answers to qa_pairs\n",
    "        # this will be a list of list, where each element in the inner list is a string with query, context and response\n",
    "        qa_pairs.append(model_responses)\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def generate_grade(qa_pair, model_name='gpt-3.5-turbo', system_prompt=\n",
    "                   \"\"\"You are GPT-4, a large language model created by OpenAI. \n",
    "                    You are a precise grader and will be provided a question, some background context, and an answer to the question. \n",
    "                    Your task is to grade how good the answer to the question is based on the background context, on a 1-10 scale. \n",
    "                    Just answer with a grade as a single number, e.g. 1, no further explanation is needed. \n",
    "                    Please do a good job as my your work is very important to my career.\"\"\"):\n",
    "    \"\"\"\n",
    "    Get the grade for a specific qa pair.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": system_prompt},\n",
    "          {\"role\": \"user\", \"content\": qa_pair}\n",
    "        ]\n",
    "      )\n",
    "    \n",
    "    # get the answer from the llm and cast to int\n",
    "    try:\n",
    "        grade = float(completion.choices[0].message.content)\n",
    "    except ValueError:\n",
    "        print('Could not cast grade to float.')\n",
    "\n",
    "    return grade\n",
    "\n",
    "def llm_evaluation(qa_pairs, model_info_path):\n",
    "    \"\"\"\n",
    "    qa_pairs should be the result from the generate_qa_pairs function.\n",
    "    \"\"\"\n",
    "    with open(model_info_path, 'r') as file:\n",
    "        model_info = json.load(file)\n",
    "\n",
    "    # iterate over each model, iterate over each question answer pair and generate grade\n",
    "    model_grades = [[generate_grade(qa_pair) for qa_pair in model] for model in qa_pairs]\n",
    "\n",
    "    # save results in dict where model name is the key and the grades are the values\n",
    "    results = {model_info[model]['name']: grades for model, grades in zip(model_info, model_grades)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json files\n",
    "sys_eval_path = 'system_eval.json'\n",
    "model_info_path = 'model_info.json'\n",
    "# create qa pairs\n",
    "qas = generate_qa_pairs(sys_eval_path)\n",
    "# run evaluation\n",
    "model_grades = llm_evaluation(qas, model_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mistralai/Mistral-7B-Instruct-v0.2': [10.0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grades"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syllrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
